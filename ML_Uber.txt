# =========================
# Cell 1: Imports
# =========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error



# =========================
# Cell 2: Load data
# =========================
# Change path if needed
df = pd.read_csv("uber.csv")

# Drop obvious non-features / duplicates
df = df.drop(columns=[c for c in df.columns if c.lower() in ["unnamed: 0"]], errors="ignore").drop_duplicates()

# Parse pickup_datetime (remove ' UTC' if present)
if "pickup_datetime" in df.columns:
    df["pickup_datetime"] = pd.to_datetime(
        df["pickup_datetime"].astype(str).str.replace(" UTC", "", regex=False),
        errors="coerce"
    )
else:
    # Some public versions carry time in 'key'
    df["pickup_datetime"] = pd.to_datetime(df["key"].str.slice(0, 19), errors="coerce")



# =========================
# Cell 3: Basic cleaning
# =========================
# Ensure numeric types (coerce invalid to NaN, then drop them)
num_cols = ["fare_amount","pickup_longitude","pickup_latitude",
            "dropoff_longitude","dropoff_latitude","passenger_count"]
for c in num_cols:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# Drop rows missing any critical column
critical = [c for c in (num_cols + ["pickup_datetime"]) if c in df.columns]
df = df.dropna(subset=critical).copy()



# =========================
# Cell 4: Feature engineering (distance + datetime parts)
# =========================
# Haversine distance (km)
def haversine(lon1, lat1, lon2, lat2):
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = np.sin(dlat/2.0)*2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)*2
    c = 2*np.arcsin(np.sqrt(a))
    return 6371.0 * c

df["distance_km"] = haversine(
    df["pickup_longitude"], df["pickup_latitude"],
    df["dropoff_longitude"], df["dropoff_latitude"]
)

# Datetime features (help models learn patterns)
df["year"]    = df["pickup_datetime"].dt.year
df["month"]   = df["pickup_datetime"].dt.month
df["day"]     = df["pickup_datetime"].dt.day
df["hour"]    = df["pickup_datetime"].dt.hour
df["weekday"] = df["pickup_datetime"].dt.weekday



# =========================
# Cell 5: Outlier rules (simple & practical)
# =========================
mask = (
    df["pickup_longitude"].between(-75, -72) &
    df["dropoff_longitude"].between(-75, -72) &
    df["pickup_latitude"].between(40, 42) &
    df["dropoff_latitude"].between(40, 42) &
    df["fare_amount"].between(2, 400) &
    df["passenger_count"].between(1, 6) &
    (df["distance_km"] > 0) & (df["distance_km"] <= 100)
)
removed_rows = (~mask).sum()
df_clean = df[mask].copy()

print("Outliers removed (rule-based):", int(removed_rows))
print("Cleaned shape:", df_clean.shape)



# =========================
# Cell 6: Correlation matrix (quick check)
# =========================
corr_features = ["fare_amount","distance_km","passenger_count","hour","weekday","month","year"]
corr_df = df_clean[corr_features].corr()
print("\nCorrelation matrix:\n", corr_df)

# Optional heatmap (uncomment to save figure)
# plt.figure(figsize=(6,5))
# im = plt.imshow(corr_df.values, interpolation='nearest')
# plt.xticks(range(len(corr_features)), corr_features, rotation=45, ha='right')
# plt.yticks(range(len(corr_features)), corr_features)
# plt.colorbar(im, fraction=0.046, pad=0.04)
# plt.title("Correlation Matrix")
# plt.tight_layout()
# plt.savefig("correlation_matrix.png", dpi=150)
# plt.close()



# =========================
# Cell 7: Train-test split
# =========================
target = "fare_amount"
feature_cols = ["distance_km","passenger_count","hour","weekday","month","year"]

X = df_clean[feature_cols].copy()
y = df_clean[target].copy()

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)



# =========================
# Cell 8: Pipelines (scaler + model)
# =========================
preprocessor = ColumnTransformer(
    transformers=[("num", StandardScaler(), feature_cols)],
    remainder="drop"
)

linreg = Pipeline(steps=[
    ("prep", preprocessor),
    ("model", LinearRegression())
])

rf = Pipeline(steps=[
    ("prep", preprocessor),  # scaling not required for trees, but harmless
    ("model", RandomForestRegressor(
        n_estimators=200,
        random_state=42,
        n_jobs=-1
    ))
])



# =========================
# Cell 9: Fit & Predict
# =========================
linreg.fit(X_train, y_train)
rf.fit(X_train, y_train)

y_pred_lin = linreg.predict(X_test)
y_pred_rf  = rf.predict(X_test)



# =========================
# Cell 10: Evaluation (R2, RMSE, MAE)
# =========================
def metrics(y_true, y_pred):
    r2 = r2_score(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    mae = mean_absolute_error(y_true, y_pred)
    return r2, rmse, mae

r2_lin, rmse_lin, mae_lin = metrics(y_test, y_pred_lin)
r2_rf,  rmse_rf,  mae_rf  = metrics(y_test, y_pred_rf)

results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest"],
    "R2": [r2_lin, r2_rf],
    "RMSE": [rmse_lin, rmse_rf],
    "MAE": [mae_lin, mae_rf]
})

print("\nModel Comparison:\n", results)

# Optional: save cleaned data
# df_clean.to_csv("uber_clean.csv", index=False)